{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # 忽略warning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.使用tf.Variable()和tf.get_variable()创建变量"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.1 使用tf.Variable()创建变量，如果name一样的话会自动处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1: <tf.Variable 'v:0' shape=(1,) dtype=float32_ref>\n",
      "v2: <tf.Variable 'v_1:0' shape=(1,) dtype=float32_ref>\n",
      "v3: <tf.Variable 'v_2:0' shape=(2,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable(initial_value=[1.0], name=\"v\")\n",
    "v2 = tf.Variable(initial_value=[2.0], name=\"v\")\n",
    "v3 = tf.Variable(initial_value=[1.0, 2.0], name=\"v\")\n",
    "print(\"v1:\", v1)\n",
    "print(\"v2:\", v2)\n",
    "print(\"v3:\", v3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.2 使用tf.get_variable()创建变量，不能使用一样的name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gv1: <tf.Variable 'gv:0' shape=(2, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "gv1 = tf.get_variable(name=\"gv\", shape=[2, 3], initializer=tf.truncated_normal_initializer())\n",
    "#v2 = tf.get_variable(name=\"gv\", shape=[2, 3], initializer=tf.truncated_normal_initializer())   变量已存在\n",
    "\n",
    "print(\"gv1:\", gv1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.3 tf.Variable()和tf.get_variable()同时创建变量，会自动处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: <tf.Variable 'var:0' shape=(1,) dtype=float32_ref>\n",
      "var2: <tf.Variable 'var_1:0' shape=(2, 3) dtype=float32_ref>\n",
      "var3: <tf.Variable 'var_2:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.Variable(initial_value=[1.0], name=\"var\", trainable=False)\n",
    "var2 = tf.get_variable(name=\"var\", shape=[2, 3], initializer=tf.truncated_normal_initializer())\n",
    "#var3 = tf.get_variable(name=\"var\", shape=[2, 3], initialzier=tf.truncated_normal_initializer())   会报错\n",
    "var3 = tf.Variable(initial_value=[2.0], name=\"var\")\n",
    "\n",
    "print(\"var1:\", var1)\n",
    "print(\"var2:\", var2)\n",
    "print(\"var3:\", var3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "如果tf.get_variable()和tf.Variable()的name冲突，会自动处理"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.4 使用tf.placeholder()创建占位符"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.placeholder()创建的并不是变量，这里只是看一下他的命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ph1: Tensor(\"ph:0\", shape=(2, 3), dtype=float32)\n",
      "ph2: Tensor(\"ph_1:0\", shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ph1 = tf.placeholder(tf.float32, shape=[2, 3], name=\"ph\")\n",
    "ph2 = tf.placeholder(tf.float32, shape=[1, 3], name=\"ph\")\n",
    "\n",
    "print(\"ph1:\", ph1)\n",
    "print(\"ph2:\", ph2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "可以看到tf.placeholder()挖了两个坑，坑名都是\"ph\"， 然后tf自动处理"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.5 获取全部的变量和trainable的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <tf.Variable 'v:0' shape=(1,) dtype=float32_ref>\n",
      "1 <tf.Variable 'v_1:0' shape=(1,) dtype=float32_ref>\n",
      "2 <tf.Variable 'v_2:0' shape=(2,) dtype=float32_ref>\n",
      "3 <tf.Variable 'gv:0' shape=(2, 3) dtype=float32_ref>\n",
      "4 <tf.Variable 'var:0' shape=(1,) dtype=float32_ref>\n",
      "5 <tf.Variable 'var_1:0' shape=(2, 3) dtype=float32_ref>\n",
      "6 <tf.Variable 'var_2:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "all_vars = tf.global_variables()\n",
    "for i in range(len(all_vars)):\n",
    "    print(i, all_vars[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <tf.Variable 'v:0' shape=(1,) dtype=float32_ref>\n",
      "1 <tf.Variable 'v_1:0' shape=(1,) dtype=float32_ref>\n",
      "2 <tf.Variable 'v_2:0' shape=(2,) dtype=float32_ref>\n",
      "3 <tf.Variable 'gv:0' shape=(2, 3) dtype=float32_ref>\n",
      "4 <tf.Variable 'var_1:0' shape=(2, 3) dtype=float32_ref>\n",
      "5 <tf.Variable 'var_2:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "all_trainable_variables = tf.trainable_variables()\n",
    "for i in range(len(all_trainable_variables)):\n",
    "    print(i, all_trainable_variables[i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.global_variables()和tf.trainable_variables()用来获取所有的变量和所有的可训练的变量\n",
    "这个在后期会用到，比如我们定义两个不同的优化器，他们各自优化部分参数，这时我们需要给他们传入各自需要优化的变量的list,这时候他们只需要优化list中的变量，其他变量就不算梯度了。\n",
    "注意上面的trainable_variables()少了一个\"var:0\"变量，因为我们把它设置成了trainable=False,那么在优化过程中就不对它计算梯度。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.使用tf.name_scope()和tf.variable_scope()管理命名空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.1 使用tf.name_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ns_v1: <tf.Variable 'ns1_1/v:0' shape=(1,) dtype=float32_ref>\n",
      "ns_gv1: <tf.Variable 'v:0' shape=(1, 2) dtype=float32_ref>\n",
      "ns_v2: <tf.Variable 'ns1_1/v_1:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"ns1\") as ns:\n",
    "    ns_v1 = tf.Variable(initial_value=[1.0], name=\"v\")\n",
    "    ns_gv1 = tf.get_variable(name=\"v\", shape=[1, 2], initializer=tf.truncated_normal_initializer())\n",
    "    ns_v2 = tf.Variable(initial_value=[1.0], name=\"v\")\n",
    "    \n",
    "print(\"ns_v1:\", ns_v1)\n",
    "print(\"ns_gv1:\", ns_gv1)\n",
    "print(\"ns_v2:\", ns_v2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "通过结果可以看出，在tf.name_scope()里：\n",
    "1.使用tf.Variable()创建的变量，会自动在前面加上scope_name\n",
    "2.使用tf.get_variable()创建的变量，并不受scope_name的影响"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.2使用tf.variable_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_v1: <tf.Variable 'vs1/v:0' shape=(1,) dtype=float32_ref>\n",
      "vs_gv1: <tf.Variable 'vs1/v_1:0' shape=(2, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"vs1\") as vs:\n",
    "    vs_v1 = tf.Variable(initial_value=[1.0], name=\"v\")\n",
    "    vs_gv1 = tf.get_variable(name=\"v\", shape=[2, 3])\n",
    "    #vs_gv2 = tf.get_variable(name=\"v\", shape=[3, 4])  #  ValueError: variable vs1/v already exists\n",
    "    \n",
    "    print(\"vs_v1:\", vs_v1)\n",
    "    print(\"vs_gv1:\", vs_gv1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "从结果可以看到tf.variabel_scope()和tf.scope_name()的区别：\n",
    "1.使用tf.Variable()创建的变量，会自动的在name前面加上variable_scope name\n",
    "2.使用tf.get_variable()创建的变量，也会自动的在name前面加上variable_scope name"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.3 tf.variable_scope()中设置reuse=True"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在开始之前，先来看看python中怎么判断两个变量指向的对象是不是一样的，直接使用is关键字就行了。不能使用==，python中的==判断的是左右两边的内容是否一样，而不是判断左右两边是不是同一个对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is b: False\n",
      "a is c: True\n",
      "a: [555, 2, 3]\n",
      "b: [1, 666, 3]\n",
      "c: [555, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [1, 2, 3]\n",
    "c = a\n",
    "\n",
    "print(\"a is b:\", a is b)\n",
    "print(\"a is c:\", a is c)\n",
    "\n",
    "c[0] = 555    #修改c，a也会变化，因为c和a指向的同一个对象\n",
    "b[1] = 666\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "上面的结果显示，a和c是指向内存中的同一个对象，如果修改c的内容，a的内容也会一起变。虽然b和a的内容一样，但他们并不是同一个对象，修改b的内容，a的内容并不会改变。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在TensorFlow中，我们是通过变量的name来区分不同的对象。tf.variable_scope()中的reuse就是为了我们实现变量共享。\n",
    "在上面我们已经创建了一个variable_scope,他的name=\"vs1\",在vs1里我们创建了一个变量，我们创建了一个变量name=\"v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_gv2: <tf.Variable 'vs1/v2:0' shape=(2, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"vs1\") as vs1:\n",
    "    vs_gv2 = tf.get_variable(name=\"v2\", shape=[2, 3])\n",
    "    # vs_gv3 = tf.get_variable(name=\"v\", shape=[3, 4])   # ValueError: variable vs1/v already exist\n",
    "    \n",
    "print(\"vs_gv2:\", vs_gv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_gv3: <tf.Variable 'vs1/v_1:0' shape=(2, 3) dtype=float32_ref>\n",
      "vs_gv1 is vs_gv3: True\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"vs1\", reuse=True) as vs1:\n",
    "    vs_gv3 = tf.get_variable(name=\"v\", shape=[2, 3])\n",
    "    \n",
    "print(\"vs_gv3:\", vs_gv3)\n",
    "print(\"vs_gv1 is vs_gv3:\", vs_gv1 is vs_gv3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "从上面可以发现，要在同一个variable_scope里定义一个已经存在的变量name=\"v\"时，需要设置reuse=True，实际上两次定义指向同一个内存对象。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3.总结：\n",
    "方式一：tf.name_scope()常常和tf.Variable()一起进行变量管理\n",
    "方式二：tf.variable_scope()常常和tf.get_variable()一起进行变量管理实现权值变量共享\n",
    "\n",
    "以后尽量使用第二种方式创建变量"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4.实用示例\n",
    "下面通过两个例子来感受一下TensorFlow的变量命名管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(X, output_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    全连接层\n",
    "    Args:\n",
    "        X: 2-D tensor, [batch_size, input_size]\n",
    "        output_size: the size of the output tensor\n",
    "    \"\"\"\n",
    "    input_size = X.shape[1]   # 特征维度\n",
    "    W = tf.get_variable(name=\"weight\", shape=[input_size, output_size], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable(name=\"bias\", shape=[output_size], initializer=tf.zeros_initializer())\n",
    "    h_fc = tf.nn.relu(tf.nn.xw_plus_b(X, W, b), name=\"relu\")\n",
    "    \n",
    "    return h_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var0 <tf.Variable 'fc1/weight:0' shape=(50, 64) dtype=float32_ref>\n",
      "var1 <tf.Variable 'fc1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "var2 <tf.Variable 'fc2/weight:0' shape=(50, 32) dtype=float32_ref>\n",
      "var3 <tf.Variable 'fc2/bias:0' shape=(32,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "feature_size = 50\n",
    "fc1_size = 64     # 第一个全连接层神经元个数\n",
    "fc2_size = 32     # 第二个全连接层神经元个数\n",
    "X_input = tf.placeholder(tf.float32, shape=[batch_size, feature_size], name=\"X_input\")\n",
    "\n",
    "with tf.variable_scope(\"fc1\") as fc1:\n",
    "    h_fc1 = fc(X_input, fc1_size)\n",
    "with tf.variable_scope(\"fc2\") as fc2:\n",
    "    h_fc2 = fc(X_input, fc2_size)\n",
    "    \n",
    "all_variables = tf.global_variables()\n",
    "for i in range(len(all_variables)):\n",
    "    print(\"var{}\".format(i), all_variables[i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "再来看一个错误的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable weight already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6ee537021bba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mh_fc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc1_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mh_fc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc2_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mall_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-18cf82c2c6e0>\u001b[0m in \u001b[0;36mfc\u001b[1;34m(X, output_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m      9\u001b[0m     \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m# 特征维度\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"weight\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bias\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mh_fc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxw_plus_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    731\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 733\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable weight already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "feature_size = 50\n",
    "fc1_size = 64     # 第一个全连接层神经元个数\n",
    "fc2_size = 32     # 第二个全连接层神经元个数\n",
    "X_input = tf.placeholder(tf.float32, shape=[batch_size, feature_size], name=\"X_input\")\n",
    "\n",
    "h_fc1 = fc(X_input, fc1_size)\n",
    "h_fc2 = fc(X_input, fc2_size)\n",
    "\n",
    "all_variables = tf.global_variables()\n",
    "for i in range(len(all_variables)):\n",
    "    print(\"var{}\".format(i), all_variables[i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在神经网络结构定义的时候，在定义每个层的时候都放在一个tf.variable_scope()里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
